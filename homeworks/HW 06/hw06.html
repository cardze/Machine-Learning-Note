<!DOCTYPE html>
<html>
<head>
<title>hw06.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<ol>
<li>We mentioned in the lecture that the perceptron (pp. 8 of back propagation) with a loss function of $\mathcal 𝐽(𝒘) = |𝑧_𝑘𝑑_𝑘 | − 𝑧_𝑘𝑑_𝑘 $ can be trained with the following updating rule: $$
𝒘(𝑘 + 1) = 𝒘(𝑘) + \begin{cases}0\space if \space z_kd_k &gt; 0, \
\mu 𝒙_𝑘𝑑_k \space if \space otherwise \
\end{cases}$$ Show that this algorithm is directly derived from the stochastic gradient descent algorithm.
<ol>
<li>Loss Function:</br>The loss function used in the perceptron algorithm is defined as: $$\mathcal J(w) = |z_k \cdot d_k| - z_k \cdot d_k$$ where $z_k$ is the activation of the perceptron's output and $d_k$ is the desired output for a given training example.</li>
<li>Gradient of the Loss Function:</br> To apply stochastic gradient descent, we need to compute the gradient of the loss function with respect to the weights (w). Let's denote the gradient as $\nabla \mathcal J(w)$.</br>The gradient of the loss function can be computed as follows: $$\nabla \mathcal J(w) = (\dfrac{\partial J}{\partial w_1}, \dfrac{\partial J}{\partial w_2}, ... ,\dfrac{\partial J}{\partial w_n}) \=
(\dfrac{\partial |z_k \cdot d_k| - z_k \cdot d_k}{\partial w_1}, \dfrac{\partial |z_k \cdot d_k| - z_k \cdot d_k}{\partial w_2}, ... ,\dfrac{\partial |z_k \cdot d_k| - z_k \cdot d_k}{\partial w_n})$$ Now, let's compute the partial derivative of the loss function with respect to each weight: $$\dfrac{\partial |z_k \cdot d_k| - z_k \cdot d_k}{\partial w_i}=\dfrac{\partial (|z_k \cdot d_k|)}{\partial w_i}-\dfrac{\partial (z_k \cdot d_k)}{\partial w_i}$$ For simplicity, let's consider a single weight $w_j$ $$\dfrac{\partial (|z_k \cdot d_k|)}{\partial w_j} = \begin{cases}-z_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w_j}\space if \space z_k \cdot d_k &lt; 0 \
z_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w_j} \space if \space z_k \cdot d_k &gt; 0 \end{cases}$$ $\dfrac{\partial (z_k \cdot d_k)}{\partial w_j}$ is simply $d_k \cdot \dfrac{\partial (z_k)}{\partial w_j}$, as $z_k$ and $d_k$ are constants with respect to $w_j$. Putting it together, we have : $$\dfrac{\partial J}{\partial w_j} = \begin{cases} -z_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w_j}, \space if \space z_k \cdot d_k &lt;0 \
0, \space if \space z_k \cdot d_k &gt;0\end{cases}$$ Thus, the gradient of the loss function with respect to each weight can be expressed as: $$\dfrac{\partial J}{\partial w_j} = -z_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w_j}, \space if \space z_k \cdot d_k &lt;0$$ $$\dfrac{\partial J}{\partial w_j} = 0, if z_k \cdot d_k &gt; 0$$</li>
<li>Updating Rule:<br>In the perceptron algorithm, the updating rule is defined as follows: $$w^{(k+1)} = w^{(k)}+ \mu \cdot x_k \cdot d_k$$ where $w^{(k)}$ is the weight vector at iteration $k$, $\mu$ is the learning rate, $x_k$ is the input vector, and $d_k$ is the desired output for the training example.</li>
<li>Comparing with SGD: <br> To derive the perceptron algorithm from the SGD algorithm, we need to compare the updating rule of the perceptron with the general form of the updating rule in SGD.<br>In SGD, the updating rule is given by:$$w^{(k+1)} = w^{(k)} - \mu \cdot \nabla J(w^{(k)})$$ Comparing this with the perceptron's updating rule, we can see that they are equivalent if we define: $$\nabla J(w) = -x_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w}$$ Since $\dfrac{\partial J}{\partial w_j} = -z_k \cdot d_k \cdot \dfrac{\partial (z_k)}{\partial w_j}$ (as derived in step 2), we can conclude that the perceptron algorithm is directly derived from the SGD algorithm by using the specific loss function and updating rule mentioned in the question.<br><br>Therefore, the perceptron algorithm can be seen as a special case of stochastic gradient descent tailored for binary classification problems.</li>
</ol>
</li>
</ol>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
         tex2jax: {inlineMath: [['$', '$'], ]}, messageStyle: "none" 
         });
</script>
</body>
</html>
